Robot Agent
===========

Fine-tuned 13B Llama model designed for ReAct-style and Tree-Of-Thoughts style prompting. The codebase has the following desirable features:

* Entire training procedure runs out of the box on a single computer with 64GB of RAM and 24GB of VRAM (i.e. consumer-grade graphics cards such as the RTX 3090 and RTX 4090) with less than 30 hours of compute time.
    * Carefully tuned to use no more than 23.6GiB of VRAM.
    * This is accomplished through quantization, FP16, TF32, and the usual gradient accumulation/checkpointing settings.
    * Training is fully interruptible/resumable.
* Heavily commented, short, clean, and reproducible training code.
    * All library dependency versions fully pinned, base models and datasets are pinned and downloaded as part of setup process.
    * After initial setup, training process does not require network access - entire project folder is portable, can be moved into airgapped and offline environments.
    * Use SafeTensors everywhere for speed and security.

Technical details:

* Based on [Llama 2 13B](https://huggingface.co/NousResearch/Llama-2-13b-hf).
* QLoRA training, a 128 rank LoRA similar to [Guanaco](https://github.com/artidoro/qlora/blob/cc488110b5ea23594a418daca7085000a9420625/qlora.py#L324).
* 2048-token context window used in supervised finetuning, 1536-token context window used in direct preference finetuning.
* Supervised finetuning using [Airoboros' self-instruct dataset](https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1), generated by [Airoboros' self-instruct implementation](https://github.com/jondurbin/airoboros).
    * The dataset has been filtered for refusals, and so could be considered "uncensored".
    * The dataset generation code also uses a GPT4 jailbreak to reduce the number of refusals in the first place.
* Direct preference finetuning using [Anthropic's hh-rlhf dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    * This replaces the reward modelling and reinforcement learning steps in a standard RLHF pipeline.
* Codebase takes ideas and inspiration from [StackLLaMa](https://github.com/lvwerra/trl/tree/5c7bfbc8d9aeabee893290cc02121d7260636978/examples/research_projects/stack_llama/scripts), [QLoRA](https://github.com/artidoro/qlora), [LLaMA-TRL](https://github.com/jasonvanf/llama-trl), [Airoboros](https://github.com/jondurbin/airoboros), .

Prompt Format
-------------

```
### Human:
INSTRUCTIONS_GO_HERE

### Assistant:
```

Note that there is a single newline at the end of the prompt. Example:

```
### Human:
What color is the sky?

### Assistant:
The sky is blue.
```

Training
--------

```
make download-datasets-and-models  # this step requires an internet connection
make train  # this step can be run fully offline, including on airgapped/offline systems, as long as the entire project folder is transferred over
```

Inference
---------

TODO: write this up, maybe export some GGMLs and GPTQs
